{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from netcal.metrics import ECE\n",
    "\n",
    "import torchbnn as bnn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_size = 64 \n",
    "\n",
    "random.seed(1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=0.5):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "train = torchvision.datasets.MNIST('./data/', train=True, download=True,\n",
    "\t\t\t\t\t\t\t\ttransform=torchvision.transforms.Compose([\n",
    "\t\t\t\t\t\t\t\t\t\ttorchvision.transforms.ToTensor(),\n",
    "\t\t\t\t\t\t\t\t\t\ttorchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "\t\t\t\t\t\t\t\t]))\n",
    "\n",
    "test = torchvision.datasets.MNIST('./data/', train=False, download=True,\n",
    "\t\t\t\t\t\t\t\ttransform=torchvision.transforms.Compose([\n",
    "\t\t\t\t\t\t\t\t\t\ttorchvision.transforms.ToTensor(),\n",
    "\t\t\t\t\t\t\t\t\t\ttorchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                        AddGaussianNoise(0., .5)\n",
    "\t\t\t\t\t\t\t\t]))\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=b_size)\n",
    "test_loader = DataLoader(test, batch_size=len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianMnistNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BayesianMnistNet, self).__init__()\n",
    "        self.conv1 = bnn.BayesConv2d(prior_mu=0., prior_sigma=.1, in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = bnn.BayesConv2d(prior_mu=0., prior_sigma=.1, in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.fc1 = bnn.BayesLinear(prior_mu=0., prior_sigma=.1, in_features=64 * 7 * 7, out_features=128)\n",
    "        self.fc2 = bnn.BayesLinear(prior_mu=0., prior_sigma=.1, in_features=128, out_features=10)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonBayesianMnistNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NonBayesianMnistNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(in_features=64 * 7 * 7, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn_model = BayesianMnistNet().to(device)\n",
    "ce = nn.CrossEntropyLoss()\n",
    "kl = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
    "kl_weight = 1.\n",
    "n_epoch = 10\n",
    "\n",
    "optimizer = optim.Adam(bnn_model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 938/938 [00:13<00:00, 68.83it/s, kl Loss=0.1253, Total Loss=1.7407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Epoch Loss: 2.7643949362133613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 938/938 [00:15<00:00, 62.37it/s, kl Loss=0.1195, Total Loss=0.3926]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Epoch Loss: 0.7217386314736755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:  99%|█████████▉| 929/938 [00:14<00:00, 63.35it/s, kl Loss=0.0975, Total Loss=0.5120]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m                     total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     28\u001b[0m                     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 30\u001b[0m                     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m                     pbar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkl Loss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkl_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m \t\t\t\t\t\t\t\t\t  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Loss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     33\u001b[0m                     pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ce_losses = []\n",
    "kl_losses = []\n",
    "losses = []\n",
    "bnn_epoch_losses = []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    bnn_model.train()\n",
    "    epoch_loss = 0.\n",
    "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch+1}/{n_epoch}', leave=True) as pbar:\n",
    "            for batch_id, sampl in enumerate(train_loader):\n",
    "                    imgs, labels = sampl\n",
    "                    imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "                    pred = bnn_model(imgs)\n",
    "                    ce_loss = ce(pred, labels)\n",
    "                    kl_loss = kl(bnn_model)\n",
    "                    total_loss = ce_loss + kl_weight*kl_loss\n",
    "                    \n",
    "                    ce_loss_cpu = ce_loss.detach().cpu().item()\n",
    "                    kl_loss_cpu = kl_loss.detach().cpu().item()\n",
    "                    total_loss_cpu = total_loss.detach().cpu().item()\n",
    "                    kl_losses.append(kl_loss_cpu)\n",
    "                    ce_losses.append(ce_loss_cpu)\n",
    "                    losses.append(total_loss_cpu)         \n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    total_loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_loss += total_loss.item()\n",
    "                    pbar.set_postfix({'kl Loss': f'{kl_loss.detach().cpu().item():.4f}',\n",
    "\t\t\t\t\t\t\t\t\t  'Total Loss': f'{total_loss.detach().cpu().item():.4f}'})\n",
    "                    pbar.update(1)\n",
    "\t\t\t\t\t\n",
    "    print(f'Epoch {epoch+1}/{n_epoch} - Epoch Loss: {epoch_loss/len(train_loader)}')\n",
    "    bnn_epoch_losses.append(epoch_loss/len(train_loader))\n",
    "\n",
    "models = []\n",
    "models.append(bnn_model)\n",
    "torch.save(bnn_model.state_dict(), f'./checkpoint/noisydata/bnn_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 938/938 [00:13<00:00, 67.09it/s, Total Loss=0.0080]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Epoch Loss: 0.23186591703721238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 938/938 [00:13<00:00, 70.63it/s, Total Loss=0.0040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Epoch Loss: 0.12146881685562697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 938/938 [00:13<00:00, 71.59it/s, Total Loss=0.0031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Epoch Loss: 0.10241588324611751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 938/938 [00:13<00:00, 71.50it/s, Total Loss=0.0220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Epoch Loss: 0.09357685144922236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 938/938 [00:13<00:00, 71.87it/s, Total Loss=0.0046]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Epoch Loss: 0.09353900067903201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 938/938 [00:13<00:00, 70.76it/s, Total Loss=0.0004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Epoch Loss: 0.08771455064255054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 938/938 [00:13<00:00, 70.28it/s, Total Loss=0.0026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Epoch Loss: 0.08654229805859413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 938/938 [00:13<00:00, 71.82it/s, Total Loss=0.0005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Epoch Loss: 0.08089353621512009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 938/938 [00:13<00:00, 69.84it/s, Total Loss=0.0670]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Epoch Loss: 0.07730662154477587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 938/938 [00:13<00:00, 68.96it/s, Total Loss=0.0001]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Epoch Loss: 0.07914280973125211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nonbnn_model = NonBayesianMnistNet().to(device)\n",
    "optimizer = optim.Adam(nonbnn_model.parameters(), lr=1e-2)\n",
    "\n",
    "nonbnn_epoch_losses = []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    nonbnn_model.train()\n",
    "    epoch_loss = 0.\n",
    "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch+1}/{n_epoch}', leave=True) as pbar:\n",
    "            for batch_id, sampl in enumerate(train_loader):\n",
    "                    imgs, labels = sampl\n",
    "                    imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "                    pred = nonbnn_model(imgs)\n",
    "                    ce_loss = ce(pred, labels)\n",
    "\n",
    "                    ce_loss_cpu = ce_loss.detach().cpu().item()\n",
    "                    ce_losses.append(ce_loss_cpu)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    ce_loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_loss += ce_loss.item()\n",
    "                    pbar.set_postfix({'Total Loss': f'{ce_loss.detach().cpu().item():.4f}'})\n",
    "                    pbar.update(1)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{n_epoch} - Epoch Loss: {epoch_loss/len(train_loader)}')\n",
    "    nonbnn_epoch_losses.append(epoch_loss/len(train_loader))\n",
    "\n",
    "torch.save(nonbnn_model.state_dict(), f'./checkpoint/noisydata/nonbnn_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m nonbnn_model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(nonbnn_state_dict_path))\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Evaluate BNN\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m bnn_accuracy, bnn_probs, bnn_labels \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBayesian Model - Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbnn_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Evaluate Non-BNN\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, test_loader)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m     12\u001b[0m     imgs, labels \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 13\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m     all_probs\u001b[38;5;241m.\u001b[39mappend(probs\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\MATH7224\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\MATH7224\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m, in \u001b[0;36mBayesianMnistNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n\u001b[0;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\MATH7224\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\MATH7224\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Bayesian Neural Network\\BayesianIris\\bayesian-neural-network-pytorch\\torchbnn\\modules\\conv.py:181\u001b[0m, in \u001b[0;36mBayesConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[0;32m    179\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_mu \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_log_sigma) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_eps\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Bayesian Neural Network\\BayesianIris\\bayesian-neural-network-pytorch\\torchbnn\\modules\\conv.py:169\u001b[0m, in \u001b[0;36mBayesConv2d.conv2d_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    164\u001b[0m     expanded_padding \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    165\u001b[0m                         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, expanded_padding, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcircular\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    167\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    168\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Evaluate models and calculate ECE using sklearn\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, all_probs, all_labels\n",
    "\n",
    "test_loader = DataLoader(test, batch_size=len(test), shuffle=False)\n",
    "\n",
    "bnn_model = BayesianMnistNet()\n",
    "nonbnn_model = NonBayesianMnistNet()\n",
    "bnn_state_dict_path = f'./checkpoint/noisydata/bnn_model.pth'\n",
    "nonbnn_state_dict_path = f'./checkpoint/noisydata/nonbnn_model.pth'\n",
    "bnn_model.load_state_dict(torch.load(bnn_state_dict_path))\n",
    "nonbnn_model.load_state_dict(torch.load(nonbnn_state_dict_path))\n",
    "\n",
    "# Evaluate BNN\n",
    "bnn_accuracy, bnn_probs, bnn_labels = evaluate_model(bnn_model, test_loader)\n",
    "print(f'Bayesian Model - Accuracy: {bnn_accuracy:.2f}%')\n",
    "\n",
    "# Evaluate Non-BNN\n",
    "nonn_accuracy, nonn_probs, nonn_labels = evaluate_model(nonbnn_model, test_loader)\n",
    "print(f'Non-Bayesian Model - Accuracy: {nonn_accuracy:.2f}%')\n",
    "\n",
    "# Plot calibration curve\n",
    "def plot_calibration_curve(probs1, labels1, probs2, labels2, title1, title2):\n",
    "    prob_true1, prob_pred1 = calibration_curve(labels1, probs1.max(axis=1), n_bins=15)\n",
    "    prob_true2, prob_pred2 = calibration_curve(labels2, probs2.max(axis=1), n_bins=15)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(prob_pred1, prob_true1, marker='o', label='Bayesian Model')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.title(title1)\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('True Probability')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(prob_pred2, prob_true2, marker='o', label='Non-Bayesian Model')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.title(title2)\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('True Probability')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_calibration_curve(bnn_probs, bnn_labels, nonn_probs, nonn_labels, \n",
    "                       'Bayesian Model Calibration Curve', 'Non-Bayesian Model Calibration Curve')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy, all_probs, all_labels\n\u001b[1;32m---> 26\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(test, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test), shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Evaluate BNN\u001b[39;00m\n\u001b[0;32m     29\u001b[0m bnn_accuracy, bnn_probs, bnn_labels \u001b[38;5;241m=\u001b[39m evaluate_model(bnn_model, test_loader)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Evaluate models and calculate ECE using sklearn\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            _, predicted = torch.max(probs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, all_probs, all_labels\n",
    "\n",
    "test_loader = DataLoader(test, batch_size=len(test), shuffle=False)\n",
    "\n",
    "# Evaluate BNN\n",
    "bnn_accuracy, bnn_probs, bnn_labels = evaluate_model(bnn_model, test_loader)\n",
    "print(f'Bayesian Model - Accuracy: {bnn_accuracy:.2f}%')\n",
    "\n",
    "# Evaluate Non-BNN\n",
    "nonn_accuracy, nonn_probs, nonn_labels = evaluate_model(nonbnn_model, test_loader)\n",
    "print(f'Non-Bayesian Model - Accuracy: {nonn_accuracy:.2f}%')\n",
    "\n",
    "# Plot calibration curve\n",
    "def plot_calibration_curve(probs1, labels1, probs2, labels2, title1, title2):\n",
    "    n_classes = 10\n",
    "    labels1_bin = label_binarize(labels1, classes=range(n_classes))\n",
    "    labels2_bin = label_binarize(labels2, classes=range(n_classes))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i in range(n_classes):\n",
    "        prob_true, prob_pred = calibration_curve(labels1_bin[:, i], probs1[:, i], n_bins=20)\n",
    "        plt.plot(prob_pred, prob_true, marker='o', label=f'Class {i}')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.title(title1)\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('True Probability')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i in range(n_classes):\n",
    "        prob_true, prob_pred = calibration_curve(labels2_bin[:, i], probs2[:, i], n_bins=20)\n",
    "        plt.plot(prob_pred, prob_true, marker='o', label=f'Class {i}')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.title(title2)\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('True Probability')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_calibration_curve(bnn_probs, bnn_labels, nonn_probs, nonn_labels, \n",
    "                       'Bayesian Model Calibration Curve', 'Non-Bayesian Model Calibration Curve')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MATH7224",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
